{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing Libraries\n",
    "Imports essential libraries like TensorFlow and Numpy for deep learning, Matplotlib and Seaborn for visualization, and Scikit-Learn for evaluation metrics.# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "import os\n",
    "import h5py as h5  # For loading HDF5 files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading Test Data\n",
    "Defines a function to load the test data from an HDF5 file, which is useful for handling large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\INTEL\\AppData\\Local\\Temp\\ipykernel_15124\\1540678359.py:3: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "  X_test = np.array(datafile['X_test'])\n",
      "C:\\Users\\INTEL\\AppData\\Local\\Temp\\ipykernel_15124\\1540678359.py:4: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "  Y_test = np.array(datafile['Y_test'])\n"
     ]
    }
   ],
   "source": [
    "def load_data(filepath):\n",
    "    with h5.File(filepath, 'r') as datafile:\n",
    "        X_test = np.array(datafile['X_test'])\n",
    "        Y_test = np.array(datafile['Y_test'])\n",
    "    return X_test, Y_test\n",
    "\n",
    "# Load the test data\n",
    "datafile = 'dataset/random-iter-1/datafile1024.h5'\n",
    "X_test, Y_test = load_data(datafile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Preprocessing\n",
    "This segment reshapes and normalizes the test data for compatibility with the model, improving performance during prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dimensions_compatible(arr):\n",
    "    return arr.reshape(arr.shape[0], -1, 1)\n",
    "\n",
    "# Preprocess the data\n",
    "X_test = make_dimensions_compatible(X_test) / 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Loading the Trained Model\n",
    "Loads the pre-trained model from a specified directory, allowing it to be used for predictions and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "model_path = 'model_directory/cnn_model'\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"Model path does not exist: {model_path}\")\n",
    "else:\n",
    "    loaded_model = tf.keras.models.load_model(model_path)\n",
    "    print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Prediction Function\n",
    "Defines a function to make predictions on the test data using the loaded model, returning predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 10ms/step\n",
      "Predictions shape: (175,)\n"
     ]
    }
   ],
   "source": [
    "def predict(X_test, model):\n",
    "    y_hat_test = model.predict(X_test)\n",
    "    return np.argmax(y_hat_test, axis=1)\n",
    "\n",
    "# Evaluate on test data\n",
    "y_hat_test = predict(X_test, loaded_model)\n",
    "print(\"Predictions shape:\", y_hat_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Calculating Accuracy\n",
    "Calculates the modelâ€™s accuracy by comparing predictions with true labels, providing a measure of overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 89.71%\n"
     ]
    }
   ],
   "source": [
    "# Load the true labels for evaluation\n",
    "Y_test_labels = np.argmax(Y_test, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(y_hat_test == Y_test_labels)\n",
    "print(\"Test Accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Confusion Matrix Creation\n",
    "Generates a confusion matrix to analyze the performance of the model across classes, helping identify areas of misclassification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "conf_matrix = confusion_matrix(Y_test_labels, y_hat_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Calculating Additional Metrics\n",
    "Calculates specificity, sensitivity, precision, and F1 score to provide a deeper evaluation of the model, especially in imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specificity: 94.29%\n",
      "Sensitivity (Recall): 90.00%\n",
      "Precision: 90.22%\n",
      "F1 Score: 89.70%\n"
     ]
    }
   ],
   "source": [
    "# True Positives, False Positives, True Negatives, False Negatives\n",
    "TP = conf_matrix[1, 1]\n",
    "TN = conf_matrix[0, 0]\n",
    "FP = conf_matrix[0, 1]\n",
    "FN = conf_matrix[1, 0]\n",
    "\n",
    "# Specificity (True Negative Rate)\n",
    "specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "\n",
    "# Sensitivity (True Positive Rate)\n",
    "sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "\n",
    "# Precision\n",
    "precision = precision_score(Y_test_labels, y_hat_test, average='weighted')\n",
    "\n",
    "# F1 Score\n",
    "f1 = f1_score(Y_test_labels, y_hat_test, average='weighted')\n",
    "\n",
    "# Print calculated metrics\n",
    "print(\"Specificity: {:.2f}%\".format(specificity * 100))\n",
    "print(\"Sensitivity (Recall): {:.2f}%\".format(sensitivity * 100))\n",
    "print(\"Precision: {:.2f}%\".format(precision * 100))\n",
    "print(\"F1 Score: {:.2f}%\".format(f1 * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
